# â„ï¸ FrozenLake Q-Learning Mini-Project

## ğŸš€ Overview
**FrozenLake Q-Learning** applies reinforcement learning to the classic `FrozenLake-v1` environment from OpenAI Gymnasium. It demonstrates how Q-Learning can solve navigation tasks in both deterministic and slippery (stochastic) settings, while investigating common pitfalls such as the **"Left-only" policy problem**.

---

## ğŸ§  Features
- ğŸ¤– **Q-Learning agent** with epsilon-greedy exploration
- ğŸ§Š Supports both deterministic (`is_slippery=False`) and slippery (`is_slippery=True`) environments
- âš ï¸ **Negative rewards** for falling into holes to encourage safer policies
- ğŸ² **Action selection noise** to break ties and improve exploration
- ğŸ“Š Rich visualizations: learning curves, policy maps, Q-table heatmaps, action distributions
- ğŸ¬ Optional animated demonstration of learned policy

---

## ğŸ“ Files
- `main.py` â€“ Main script for training, analysis, and visualization  
- `main.ipynb` â€“ Jupyter notebook version (if available)  
- `frozen_lake_results.png` â€“ Learning curve visualization  
- `frozenlake_extended_analysis.png` â€“ Key plots 
- `frozenlake_extended_comparison-analysis.png` â€“ Key plots for comparing slippery and non-slippery environments


## â–¶ï¸ How to Run ##
- Clone the repository or copy the files to your workspace.
Run main.py to train the agent and generate visualizations.
Review the console output and generated plots for analysis.

---

## How to Run ##
- Clone the repository or copy the files to your workspace.
Run main.py to train the agent and generate visualizations.
Review the console output and generated plots for analysis.

---

## ğŸ” Key Insights
- Penalizing holes and adding noise to action selection improves policy diversity and success rate in slippery environments.
- The "Left-only" problem is caused by lack of penalty for holes and insufficient exploration.
- The same hyperparameters can yield different results depending on environment stochasticity and reward structure.

---
## ğŸ“ˆ Results ##
- **Deterministic environment:** High success rate (â‰ˆ93.5%), efficient policy.
- **Slippery environment:** Lower success rate (â‰ˆ50.8%), but improved action diversity and robustness.
- **Visualizations** show learning progress, policy maps, and Q-table values.

---

## ğŸ“š Resources ##
- Online tutorials
- Matplotlib and Seaborn for visualization

## ğŸ¤ Presentation Link
[Check out the full presentation](https://docs.google.com/presentation/d/162wKMr2CtW1aMuWJoyaSJ0sdbp5E307RBWrbrfXdF90/edit?usp=sharing)

---

## ğŸ› ï¸ Requirements
- Python 3.8+
- `gymnasium`
- `numpy`
- `matplotlib`
- `seaborn`
- `tqdm`

Install dependencies with:
```bash
pip install gymnasium numpy matplotlib seaborn tqdm




